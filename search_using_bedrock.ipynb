{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is Machine vision detection to daily facial fatigue ?\"\n",
    "CATEGORY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/Ambarish/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from qdrant_helper import insert_qdrant_docs, \\\n",
    "    get_search_results\n",
    "import qdrant_client as qc\n",
    "import qdrant_client.http.models as qmodels\n",
    "from qdrant_client.http.models import *\n",
    "from model_helper import load_model\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qdrant_client():\n",
    "    client = qc.QdrantClient(url=URL)\n",
    "    METRIC = qmodels.Distance.COSINE\n",
    "    return client\n",
    "\n",
    "\n",
    "client = get_qdrant_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "xq = model.encode(user_input,convert_to_tensor=True)\n",
    "if CATEGORY == '':\n",
    "    query_filter = None\n",
    "else:\n",
    "    query_filter=qmodels.Filter(\n",
    "        must= [\n",
    "            FieldCondition(\n",
    "                key=\"Category\",\n",
    "                match=models.MatchValue(value=CATEGORY),\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "search_result = client.search(collection_name=COLLECTION_NAME,\n",
    "                                query_vector=xq.tolist(), \n",
    "                                query_filter=query_filter,\n",
    "                                limit=3)\n",
    "contexts =\"\"\n",
    "for result in search_result:\n",
    "    contexts +=  result.payload['abstract']+\"\\n---\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('  Fatigue detection is valued for people to keep mental health and prevent\\n'\n",
      " 'safety accidents. However, detecting facial fatigue, especially mild fatigue '\n",
      " 'in\\n'\n",
      " 'the real world via machine vision is still a challenging issue due to lack '\n",
      " 'of\\n'\n",
      " 'non-lab dataset and well-defined algorithms. In order to improve the '\n",
      " 'detection\\n'\n",
      " 'capability on facial fatigue that can be used widely in daily life, this '\n",
      " 'paper\\n'\n",
      " 'provided an audiovisual dataset named DLFD (daily-life fatigue dataset) '\n",
      " 'which\\n'\n",
      " \"reflected people's facial fatigue state in the wild. A framework using\\n\"\n",
      " '3D-ResNet along with non-local attention mechanism was training for '\n",
      " 'extraction\\n'\n",
      " 'of local and long-range features in spatial and temporal dimensions. Then, '\n",
      " 'a\\n'\n",
      " 'compacted loss function combining mean squared error and cross-entropy was\\n'\n",
      " 'designed to predict both continuous and categorical fatigue degrees. Our\\n'\n",
      " 'proposed framework has reached an average accuracy of 90.8% on validation '\n",
      " 'set\\n'\n",
      " 'and 72.5% on test set for binary classification, standing a good position\\n'\n",
      " 'compared to other state-of-the-art methods. The analysis of feature map\\n'\n",
      " 'visualization revealed that our framework captured facial dynamics and\\n'\n",
      " 'attempted to build a connection with fatigue state. Our experimental results '\n",
      " 'in\\n'\n",
      " 'multiple metrics proved that our framework captured some typical, micro and\\n'\n",
      " 'dynamic facial features along spatiotemporal dimensions, contributing to '\n",
      " 'the\\n'\n",
      " 'mild fatigue detection in the wild.\\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '  Around 40 percent of accidents related to driving on highways in India '\n",
      " 'occur\\n'\n",
      " 'due to the driver falling asleep behind the steering wheel. Several types '\n",
      " 'of\\n'\n",
      " 'research are ongoing to detect driver drowsiness but they suffer from the\\n'\n",
      " 'complexity and cost of the models. In this paper, SleepyWheels a '\n",
      " 'revolutionary\\n'\n",
      " 'method that uses a lightweight neural network in conjunction with facial\\n'\n",
      " 'landmark identification is proposed to identify driver fatigue in real '\n",
      " 'time.\\n'\n",
      " 'SleepyWheels is successful in a wide range of test scenarios, including the\\n'\n",
      " 'lack of facial characteristics while covering the eye or mouth, the drivers\\n'\n",
      " 'varying skin tones, camera placements, and observational angles. It can '\n",
      " 'work\\n'\n",
      " 'well when emulated to real time systems. SleepyWheels utilized '\n",
      " 'EfficientNetV2\\n'\n",
      " 'and a facial landmark detector for identifying drowsiness detection. The '\n",
      " 'model\\n'\n",
      " 'is trained on a specially created dataset on driver sleepiness and it '\n",
      " 'achieves\\n'\n",
      " 'an accuracy of 97 percent. The model is lightweight hence it can be further\\n'\n",
      " 'deployed as a mobile application for various platforms.\\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '  Driver monitoring systems (DMS) are a key component of vehicular safety '\n",
      " 'and\\n'\n",
      " 'essential for the transition from semiautonomous to fully autonomous '\n",
      " 'driving. A\\n'\n",
      " 'key task for DMS is to ascertain the cognitive state of a driver and to\\n'\n",
      " 'determine their level of tiredness. Neuromorphic vision systems, based on '\n",
      " 'event\\n'\n",
      " 'camera technology, provide advanced sensing of facial characteristics, in\\n'\n",
      " \"particular the behavior of a driver's eyes. This research explores the\\n\"\n",
      " 'potential to extend neuromorphic sensing techniques to analyze the entire\\n'\n",
      " 'facial region, detecting yawning behaviors that give a complimentary '\n",
      " 'indicator\\n'\n",
      " 'of tiredness. A neuromorphic dataset is constructed from 952 video clips '\n",
      " '(481\\n'\n",
      " 'yawns, 471 not-yawns) captured with an RGB color camera, with 37 subjects. '\n",
      " 'A\\n'\n",
      " 'total of 95200 neuromorphic image frames are generated from this video data\\n'\n",
      " 'using a video-to-event converter. From these data 21 subjects were selected '\n",
      " 'to\\n'\n",
      " 'provide a training dataset, 8 subjects were used for validation data, and '\n",
      " 'the\\n'\n",
      " 'remaining 8 subjects were reserved for an \"unseen\" test dataset. An '\n",
      " 'additional\\n'\n",
      " '12300 frames were generated from event simulations of a public dataset to '\n",
      " 'test\\n'\n",
      " 'against other methods. A CNN with self-attention and a recurrent head was\\n'\n",
      " 'designed, trained, and tested with these data. Respective precision and '\n",
      " 'recall\\n'\n",
      " 'scores of 95.9 percent and 94.7 percent were achieved on our test set, and '\n",
      " '89.9\\n'\n",
      " 'percent and 91 percent on the simulated public test set, demonstrating the\\n'\n",
      " 'feasibility to add yawn detection as a sensing component of a neuromorphic '\n",
      " 'DMS.\\n'\n",
      " '\\n'\n",
      " '---\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client(service_name='bedrock-runtime',\n",
    "                             region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(context,query):\n",
    "    \n",
    "    context2 = f\"\"\"\n",
    "Human: {context}  {query} \n",
    "Assistant:\"\"\"\n",
    "    return  context2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = create_prompt(contexts,user_input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHuman:   Fatigue detection is valued for people to keep mental health and prevent\\nsafety accidents. However, detecting facial fatigue, especially mild fatigue in\\nthe real world via machine vision is still a challenging issue due to lack of\\nnon-lab dataset and well-defined algorithms. In order to improve the detection\\ncapability on facial fatigue that can be used widely in daily life, this paper\\nprovided an audiovisual dataset named DLFD (daily-life fatigue dataset) which\\nreflected people\\'s facial fatigue state in the wild. A framework using\\n3D-ResNet along with non-local attention mechanism was training for extraction\\nof local and long-range features in spatial and temporal dimensions. Then, a\\ncompacted loss function combining mean squared error and cross-entropy was\\ndesigned to predict both continuous and categorical fatigue degrees. Our\\nproposed framework has reached an average accuracy of 90.8% on validation set\\nand 72.5% on test set for binary classification, standing a good position\\ncompared to other state-of-the-art methods. The analysis of feature map\\nvisualization revealed that our framework captured facial dynamics and\\nattempted to build a connection with fatigue state. Our experimental results in\\nmultiple metrics proved that our framework captured some typical, micro and\\ndynamic facial features along spatiotemporal dimensions, contributing to the\\nmild fatigue detection in the wild.\\n\\n---\\n  Around 40 percent of accidents related to driving on highways in India occur\\ndue to the driver falling asleep behind the steering wheel. Several types of\\nresearch are ongoing to detect driver drowsiness but they suffer from the\\ncomplexity and cost of the models. In this paper, SleepyWheels a revolutionary\\nmethod that uses a lightweight neural network in conjunction with facial\\nlandmark identification is proposed to identify driver fatigue in real time.\\nSleepyWheels is successful in a wide range of test scenarios, including the\\nlack of facial characteristics while covering the eye or mouth, the drivers\\nvarying skin tones, camera placements, and observational angles. It can work\\nwell when emulated to real time systems. SleepyWheels utilized EfficientNetV2\\nand a facial landmark detector for identifying drowsiness detection. The model\\nis trained on a specially created dataset on driver sleepiness and it achieves\\nan accuracy of 97 percent. The model is lightweight hence it can be further\\ndeployed as a mobile application for various platforms.\\n\\n---\\n  Driver monitoring systems (DMS) are a key component of vehicular safety and\\nessential for the transition from semiautonomous to fully autonomous driving. A\\nkey task for DMS is to ascertain the cognitive state of a driver and to\\ndetermine their level of tiredness. Neuromorphic vision systems, based on event\\ncamera technology, provide advanced sensing of facial characteristics, in\\nparticular the behavior of a driver\\'s eyes. This research explores the\\npotential to extend neuromorphic sensing techniques to analyze the entire\\nfacial region, detecting yawning behaviors that give a complimentary indicator\\nof tiredness. A neuromorphic dataset is constructed from 952 video clips (481\\nyawns, 471 not-yawns) captured with an RGB color camera, with 37 subjects. A\\ntotal of 95200 neuromorphic image frames are generated from this video data\\nusing a video-to-event converter. From these data 21 subjects were selected to\\nprovide a training dataset, 8 subjects were used for validation data, and the\\nremaining 8 subjects were reserved for an \"unseen\" test dataset. An additional\\n12300 frames were generated from event simulations of a public dataset to test\\nagainst other methods. A CNN with self-attention and a recurrent head was\\ndesigned, trained, and tested with these data. Respective precision and recall\\nscores of 95.9 percent and 94.7 percent were achieved on our test set, and 89.9\\npercent and 91 percent on the simulated public test set, demonstrating the\\nfeasibility to add yawn detection as a sensing component of a neuromorphic DMS.\\n\\n---\\n  What is Machine vision detection to daily facial fatigue ? \\nAssistant:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\"prompt\": prompt_data, \"max_tokens_to_sample\": 500})\n",
    "modelId = \"anthropic.claude-instant-v1\"  \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = boto3_bedrock.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Machine vision detection of daily facial fatigue refers to the use of '\n",
      " 'computer vision and machine learning techniques to automatically detect and '\n",
      " 'assess levels of fatigue from facial images of people in daily, real-world '\n",
      " 'settings. Some key points:\\n'\n",
      " '\\n'\n",
      " '- The goal is to detect more mild or subtle forms of fatigue that may occur '\n",
      " 'during everyday activities, as opposed to obvious fatigue measurable in '\n",
      " 'controlled lab settings. This is more challenging due to uncontrolled '\n",
      " 'variables.\\n'\n",
      " '\\n'\n",
      " '- Facial fatigue is assessed based on subtle changes in facial features, '\n",
      " 'expressions, eye gaze, etc. over time that may indicate increasing tiredness '\n",
      " 'or exhaustion. \\n'\n",
      " '\\n'\n",
      " '- Deep learning models, especially convolutional neural networks, are '\n",
      " 'commonly used to analyze facial images and videos to learn the visual cues '\n",
      " 'associated with different fatigue levels.\\n'\n",
      " '\\n'\n",
      " '- Spatial features within individual frames as well as temporal changes '\n",
      " 'across frames can provide clues. Models may incorporate attention mechanisms '\n",
      " 'to focus on key regions.\\n'\n",
      " '\\n'\n",
      " \"- Specially collected datasets showing people's faces in daily conditions \"\n",
      " 'are needed for realistic training and evaluation of models, since lab '\n",
      " \"datasets don't reflect real-world variability.\\n\"\n",
      " '\\n'\n",
      " '- Accuracy can be improved by using additional modalities like audio/voice '\n",
      " 'along with vision, or 3D modeling to capture subtle facial dynamics better.\\n'\n",
      " '\\n'\n",
      " '- The goal is to develop techniques that can be used in applications like '\n",
      " 'monitoring driver alertness, evaluating work/study engagement, or detecting '\n",
      " 'healthcare needs.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response_body[\"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
